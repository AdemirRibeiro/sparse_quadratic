\documentclass[smallextended,referee,envcountsect]{svjour3}
% The option smallextended is the standard JOTA format.
% The option referee  makes the paper double-spaced.
% The option envcountsect numbers theorems, etc, by section.
% svjour3 is the document class for Springer journals.
\smartqed
%This command right justifies \qed throughout the paper.

\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{color}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{url}
%\usepackage{tkz-berge}
\usepackage{algorithm}
\usepackage{algorithmic}
% \newtheorem{corollary}[theorem]{Corollary}% 

\journalname{Optimization Letters}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

\definecolor{Ademir}{RGB}{10, 10, 250}
\definecolor{Mael}{RGB}{250, 10, 250} 
\definecolor{rev}{RGB}{0, 0, 255} 
\newcommand{\Ademir}[1]{{\color{Ademir} #1}} % inclusoes feitas por Ademir
\newcommand{\Mael}[1]{{\color{Mael} #1}} % inclusoes feitas pela Mael
\newcommand{\rmv}[1]{{\color{red} #1}} % coisas para remover 
\newcommand{\rev}[1]{{\color{rev} #1}} % revisao



\begin{document}

\title{Sparse quadratic optimization via cardinality constraints}

\titlerunning{Sparse optimization via cardinality constraints}

\author{Mael Sachine \and Ademir A. Ribeiro
% \footnote{This author was partially supported by CNPq (grant 307987/2023-0).}
}

\authorrunning{M. Sachine and A. A. Ribeiro} 

\institute{Mael Sachine, mael@ufpr.br \\ 
Ademir A. Ribeiro, ademir.ribeiro@ufpr.br (Corresponding author) 
\at Federal University of Paran\'a - PR, Brazil
}

\date{Received: date / Accepted: date}

\maketitle

\begin{abstract}
We study the sparse regularized quadratic optimization problem where the sparsity level 
is controlled by means of the cardinality constraints. However, due to the 
existence of this kind of constraint (which is not continuous neither convex), it is 
very difficult to directly solve the problem. Recently, several researchers have been 
addressed these type of problems using a common strategy, the continuous relaxation 
reformulation of the problem. In this paper we propose a different approach in which the 
feasible set of the problem is decomposed into simpler sets, all of them meeting the 
cardinality constraint. Then, in order to overcome the combinatorial difficulty induced 
by this decomposition, we make use of an auxiliary and simple problem of maximizing a 
concave piecewise quadratic function. The solution of this problem, obtained by a 
subgradient method, is then used to find the solution of the original problem. 
Numerical experiments show that this strategy may be successful for a significant 
number of problems. Throughout this paper we establish nonlinear optimization results 
in order to provide a rigorous mathematical analysis of the ideas involving the 
reformulation of problem, the proposed method and its convergence properties.
\end{abstract}
\keywords{Nonlinear programming \and Mathematical programs with cardinality 
constraints \and Sparse optimization \and Sparse quadratic problems 
\and Sparse regression}
\subclass{90C30 \and 90C46 \and 62J05}


\section{Introduction}
\label{sec:intro}
In this paper we consider the regularized quadratic problem of the form 
\begin{equation}
\label{problem_x}
\begin{array}{cl}
\mathop{\rm minimize } & c^Tx+x^TQx+\frac{1}{\eta}\|x\|^2 \\ 
{\rm subject\ to } & Ax\leq b, \\ 
& \|x\|_0\leq s,
\end{array}
\end{equation}
where $Q\in\R^{n\times n}$ is symmetric positive semidefinite, $c\in\R^n$, $\eta>0$, 
$A\in\R^{m\times n}$, $b\in\R^m$, $s\in\{1,\ldots,n-1\}$ and $\|x\|_0$ denotes the 
number of nonzero components of $x$ and is referred to as {\em cardinality} of $x$. 
The ridge regularization term in the objective function is widely used and 
helps to reduce the effect of noise in the data and also makes the problem more 
tractable and robust \cite{BertsimasCory-Wright22,BertsimasParys20,HoerlKennard70,Tikhonov43}.

The task of minimizing a convex quadratic function is equivalent to a linear regression 
problem (see Lemma \ref{lm:quad_least_squares} below), 
which is also known in the optimization community as least squares 
problem \cite{HastieTibshiraniFriedman08,MontgomeryPeckVining21}. 
This problem is a classical tool in various applications, such as 
subset selection \cite{Miller02,Tibshirani}, 
compressed sensing \cite{Donoho06,EldarKutyniok12}, 
portfolio selection \cite{BertsimasCory-Wright22,Markowitz52} 
and machine learning \cite{BennettMangasarian92,CristianiniShaweTaylor00,Mangasarian}.
In fact, in order to make the modeling more realistic, encompassing various real-world 
situations, the constraints $Ax\leq b$ and $\|x\|_0\leq s$ are considered.

The cardinality constraint aims to control the sparsity level of the solution. 
Although this challenging constraint is not continuous neither convex, which makes the 
problem difficult to be directly solved, it has gained prominence in the literature due 
to its relevance in many applications involving sparse optimization 
\cite{BeckEldar,BeckHallak16,BurdakovKanzowSchwartz16,KanzowRaharjaSchwartz21a,KrejicKrulikovskiRaydan23,KrulikovskiRibeiroSachine21,MedeirosRibeiroSachineSecchin24,RaydanKrejicKrulikovski23,RibeiroSachineKrulikovski22}. 
Many of these works reformulate the problem by using a continuous relaxation. 
We mention that the sparsity is also addressed with other strategies 
(see LASSO \cite{Tibshirani} and Elastic Net \cite{ZouHastie05}, to name a few).

\newpage

\Ademir{In this paper we study the sparse regularized quadratic 
problem \eqref{problem_x}, by replacing the set of points that satisfies 
the cardinality constraint by a combinatorial union of simpler sets of the 
form $\{x\in\R^n\mid{x_I}=0\}$, where the index set $I$ percorre as possibilidades 
satisfazendo $|I|=n-{s}$. By making this decomposition we get a combinatorial number 
of problems to be solved. 
In order to avoid to run all the problems 
we first solve a very cheap problem, which will provide a lower bound for the optimal 
value of the original problem. 
We then start to solve these combinatorial quantity of problems with the hope that the 
value coincides with the bound. If this bound is reached, we have found the optimal value of problem of \eqref{problem_minimin} without the need to solve 
all the $N:=\left(\begin{array}{c} n \\ s \end{array}\right)$ problems. 

providing a comprehensive analysis in many aspects: 
theoretical optimization results regarding the problem, which allows us to establish 
existence of solutions of the involved problems, obtain the dual problem by using the 
Lagrangian relaxation, etc. 



\noindent\rmv{ The solution of this problem, obtained by a 
subgradient method, is then used to find the solution of the original problem. 
Numerical experiments show that this strategy may be successful for a significant 
number of problems. Throughout this paper we establish nonlinear optimization results 
in order to provide a rigorous mathematical analysis of the ideas involving the 
reformulation of problem, the proposed method and its convergence properties..}


\Mael{

Then, y induced 
by this decomposition, we make use of an auxiliary and simple problem of maximizing a 
concave piecewise quadratic function. The solution of this problem, obtained by a 
subgradient method, is then used to find the solution of the original problem.

using a 
In this paper we propose a different approach in which the 
feasible set of the problem is decomposed into simpler sets, all of them meeting the 
cardinality constraint. Then, in order to overcome the combinatorial difficulty induced 
by this decomposition, we make use of an auxiliary and simple problem of maximizing a 
concave piecewise quadratic function. The solution of this problem, obtained by a 
subgradient method, is then used to find the solution of the original problem. 
}

Other researchers also deal with this problem with the goal of solving large-scale 
problems \cite{BertsimasCory-Wright22,BertsimasKingMazumder16,BertsimasParys20,VreugdenhilNguyenEftekhariEsfahani21}.

However, the scope of this paper is theoretical study, ...

}

The paper is organized as follows: Sect. \ref{sec:general_nlp} provides a 
collection of general nonlinear optimization results, established to be the foundations 
to the rest of the paper. In Sect. \ref{sec:sparse_problem} we discuss the problem which 
is the subject of this study and its reformulation in order to tackle cardinality 
constraints. Sect. \ref{sec:maxmin_problem} is devoted to discuss an auxiliary problem 
used to obtain a lower bound on the optimal value of the original problem. In 
Sect. \ref{sec:proposed_approach} we propose a strategy for solving our problem, whose 
numerical experiments are presented in Sect. \ref{sec:numerical_experiments}. 
Conclusions are presented in Sect. \ref{sec:conclusions}.

\medskip

{\noindent\bf Notation.} Throughout this paper, we use $\|\cdot\|$ to denote the 
Euclidean norm. For a vector $x\in\R^n$, $\|x\|_0$ is the cardinality of $x$, that is, 
the number of nonzero components of $x$. The $i$th unit vector of $\R^n$, that is, 
the vector whose elements are all $0$ except for a $1$ in the $i$th position is denoted 
by $e_i$. Given $z\in\R^n$, ${\rm diag}(z)$ is the $n\times n$ diagonal matrix with 
diagonal $z$. For a natural number $r\in\N$, denote $[r]=\{1,\ldots,r\}$ and for an 
index set $I\subset[n]$ such that $|I|=r$ we write $x_I\in\R^r$ to denote the vector 
whose components are $x_i$, $i\in I$. \Ademir{The nullspace and range of a matrix 
$A\in\R^{m\times n}$ are represented by ${\mathcal{N}}(A)$ and 
${\mathcal{R}}(A)$, respectively.}


\section{Nonlinear optimization results}
\label{sec:general_nlp}
In this section, we establish results that, apart from their own significance, 
also provide a mathematical foundation for the rest of the paper. First, we prove some 
properties on quadratic and piecewise quadratic functions. Then, we discuss duality in a more 
general setting, namely, Lagrangian relaxation. Finally, we relate infeasibility of a 
certain problem with unboundedness of the dual.


\subsection{Quadratic and piecewise quadratic functions}
\label{sec:basics}
\begin{lemma}
\label{lm:quad_least_squares}
Consider $H\in\R^{n\times n}$ symmetric, $c\in\R^n$, $d\in\R$ and the quadratic function 
$f:\R^n\to\R$ given by $f(x)=x^THx+c^Tx+d$. Suppose that $f$ is bounded below. Then $H$ 
is positive semidefinite and $f$ has a global minimizer. Moreover, minimizing $f$ is 
equivalent to a linear regression problem, that is, a least squares problem. 
\end{lemma}
\begin{proof}
First, note that if $\lambda$ is an eigenvalue of $H$ and $v\neq 0$ is a 
corresponding eigenvector, then 
$$
f(tv)=t^2\lambda v^Tv+tc^Tv+d.
$$
Since $f$ is bounded below, we have $\lambda\geq 0$. Now, given $w\in{\mathcal {N}}(H)$, 
there holds $f(tw)=tc^Tw+d$ and hence (using again the boundedness of $f$), $c^Tw=0$. 
This implies that $-c\in{\mathcal {N}}(H)^\perp={\mathcal{R}}(H^T)={\mathcal{R}}(H)$, that is, 
there exists $x^*\in\mathbb{R}^n$ such that $\nabla f( x^*)=Hx^*+c=0$. The convexity 
of $f$ then ensures that $x^*$ is a global minimizer of $f$. 

For the last statement, noting that $H$ is positive semidefinite, its eigenvalues 
$\lambda_1\geq\lambda_2\geq\cdots\geq\lambda_n\geq 0$ and their associated (orthonormal) 
eigenvectors $v_1, v_2, \ldots, v_n$ can be used to write a spectral decomposition 
$$
H = V\Lambda V^T = WW^T,
$$
where $V=(v_1\;\cdots\;v_n)$, $\Lambda={\rm diag}(\lambda_1,\ldots,\lambda_n)$ 
and $W = V\sqrt{\Lambda}$. Thus, defining $\xi = \frac{1}{2}W^Tx^*$ and 
$\zeta = d -\|\xi\|^2$, we have 
$$
f(x) = \|W^Tx - \xi\|^2 + \zeta,
$$
% \rmv{Note que $2W\xi=WW^Tx^*=Hx^*=-c$ e assim, 
% $$
% \|W^Tx - \xi\|^2 + \zeta = x^TWW^Tx - 2x^TW\xi + d = x^THx + c^Tx + d.
% $$}
completing the proof.
\qed\end{proof}

Consider $A\in\R^{m\times n}$, $b\in\R^m$ and $\gamma:\R^n\to\R^m$ defined by 
$\gamma(x)=Ax+b$. Given $z\in\R_+^m$, denote $Z={\rm diag}(z)$ and define the function 
$\varphi_z:\R^n\to\R$ by 
\begin{equation}
\label{fc_gmzgm}
\varphi_z(x)=\gamma(x)^TZ\gamma(x).
\end{equation}

\begin{lemma}
\label{lm:gmzgm}
The function $\varphi_z$ defined in \eqref{fc_gmzgm} is quadratic and convex. 
\end{lemma}
\begin{proof}
Denoting $P=\sqrt{Z}A$ and $q=\sqrt{Z}b$, we have 
$$
\varphi_z(x)=\|\sqrt{Z}\gamma(x)\|^2=\|Px+q\|^2=x^TP^TPx+2q^TPx+q^Tq,
$$
which is clearly quadratic. Moreover, $\nabla^2\varphi_z(x)=2P^TP$ is positive 
semideﬁnite since $v^T{P^TP}v=\|Pv\|^2\geq 0$. So, $\varphi_z$ is also convex.
\qed\end{proof}

\begin{lemma}
\label{lm:max_gmzgm}
Let $D\subset\R_+^m$ be a finite set. Then the function $g:\R^n\to\R$ defined by 
\begin{equation}
\label{fc_max_gmzgm}
g(x)=\max_{z\in D}\varphi_z(x)
\end{equation}
is continuous, convex and piecewise quadratic. 
\end{lemma}
\begin{proof}
Since $D$ is finite, say $|D|=r$, we can denote the functions $\varphi_z$, $z\in D$, 
by $\varphi_1,\ldots,\varphi_r$. Let $x^k\to\bar{x}$ and 
$$
\N_i=\{k\in\N\mid g(x^k)=\varphi_i(x^k)\}.
$$
Then $\varphi_i(x^k)\geq\varphi_j(x^k)$ for all $j\in[r]$, which gives 
$\varphi_i(\bar{x})\geq\varphi_j(\bar{x})$. So, for $k\in\N_i$, we have 
$$
g(x^k)=\varphi_i(x^k)\stackrel{\N_i}{\to}\varphi_i(\bar{x})=g(\bar{x}).
$$
Since $\N=\displaystyle\bigcup_{i=1}^{r}\N_i$, we have $g(x^k)\to g(\bar{x})$, proving 
that $g$ is continuous. Now, given $x, y\in\R^n$ and $t\in[0,1]$, there exists 
$i\in[r]$ such that  
$$
g\big((1-t)x+ty\big)=\varphi_i\big((1-t)x+ty\big)\leq(1-t)\varphi_i(x)+t\varphi_i(y)
\leq(1-t)g(x)+tg(y),
$$
which means that $g$ is convex. Finally, note that $\R^n$ can be decomposed as 
$$
\R^n=\displaystyle\bigcup_{i=1}^{r}X_i,
$$
where 
$$
X_i=\{x\in\R^n\mid g(x)=\varphi_i(x)\}=
\{x\in\R^n\mid \varphi_i(x)\geq\varphi_j(x), \forall j\in[r]\}.
$$
Thus, $g$ is quadratic on each region $X_i$, $i\in[r]$.
\qed\end{proof}

Now, let us discuss some points concerning the existence of solutions of a optimization 
problem. First, note that convexity alone is not enough to guarantee existence of a 
solution. Indeed, the problem of minimizing the quadratic convex function $x_1^2+x_2$ on 
$\Omega=\{x\in\R^2\mid x_1+x_2-1\leq 0\}$ does not have a solution. Moreover, even if the 
objective function is bounded below, we cannot prove the existence, as we can see if we 
want to minimize the convex function $e^{x_1+x_2}$ on $\Omega$. Polynomials bounded below 
may also have the same problem: $(x_1x_2-1)^2+x_1^2$ on $\Omega$ (take, for example, 
the sequence $x^k=-(1/k,k)$).

However, if the objective function is quadratic and bounded below on a polyhedral 
feasible set, it is guaranteed the existence of a solution \cite{BlumOettli72,FrankWolfe56}. 
In fact, we establish next a more general result extending this property to piecewise 
quadratic functions. For convenience, we shall consider the problem of 
maximizing a piecewise quadratic function which is bounded above.

\begin{theorem}
\label{th:exist_piecewise_quad}
Let $r\in\N$ and consider, for each $j\in[r]$, $H_j\in\R^{n\times n}$ symmetric, 
$c_j\in\R^n$, $d_j\in\R$ and the quadratic functions $f_j:\R^n\to\R$ given by 
$f_j(x)=x^TH_jx+c_j^Tx+d_j$. Define $f:\R^n\to\R$ by 
$$
f(x)=\min_{j\in[r]}f_j(x)
$$
and consider $A\in\R^{m\times n}$, $b\in\R^m$. If the functions $f_j$, $j\in[r]$, are 
bounded above on the set $\Omega=\{x\in\R^n\mid Ax+b\leq 0\}$, then the problem 
$$
\begin{array}{cl}
\displaystyle{\rm maximize } & f(x) \\
{\rm subject\ to} & x\in\Omega
\end{array}
$$
has a global solution.
\end{theorem}
\begin{proof}
Denote $\bar\rho=\min\{\|x\|\mid x\in\Omega\}$, 
$\Omega_\rho=\{x\in\Omega\mid\|x\|\leq\rho\}$, for $\rho\geq\bar\rho$ and $x_\rho$ a 
minimum norm global solution\footnote{Note that the set of global solutions of an 
optimization problem is closed and hence has a point of minimum norm.} of the problem 
$$
\begin{array}{cl}
\displaystyle{\rm maximize } & f(x) \\
{\rm subject\ to} & x\in\Omega_\rho.
\end{array}
$$
We claim that there exists $\tilde\rho\geq\bar\rho$ such that 
\begin{equation}
\label{xrho_inactive}
\|x_\rho\|<\rho
\end{equation}
for all $\rho\geq\tilde\rho$. We prove this by contradiction, by assuming that there 
exists a sequence $(\rho_k)_{k\in\N}$ such that $\rho_k\to\infty$ and 
$\|x_{\rho_k}\|=\rho_k$. Defining $x^k=x_{\rho_k}$ and $u^k=\dfrac{x^k}{\rho_k}$, we 
have (without loss of generality) that $u^k\to u$, with $\|u\|=1$. Denote the rows of 
$A$ by $a_i^T$ and consider the index set 
$$
I_0=\Big\{i\in[m]\mid\limsup_{k\to\infty}a_i^Tx^k+b_i=0\Big\}.
$$ 
For $i\notin I_0$ we have $\displaystyle\limsup_{k\to\infty}a_i^Tx^k+b_i<0$ or 
$\displaystyle\lim_{k\to\infty}a_i^Tx^k+b_i=-\infty$. So, discarding finitely many 
entries of the sequence $(x^k)_{k\in\N}$ if necessary, we conclude that there exists 
$\epsilon>0$ such that 
\begin{equation}
\label{bd_away}
a_i^Tx^k+b_i\leq-\varepsilon,
\end{equation}
for all $i\notin I_0$ and $k\in\N$. Note that $a_i^Tx^k+b_i=\rho_ka_i^Tu^k+b_i$ and 
$a_i^Tu^k\to a_i^Tu$ for all $i\in[m]$. Thus, since $\rho_k\to\infty$, there holds 
\begin{equation}
\label{u_rec}
a_i^Tu=0,\; i\in I_0 \quad\mbox{and}\quad a_i^Tu\leq 0,\; i\notin I_0.
\end{equation}
Hence, $x^k+\lambda u\in\Omega$ for all $k\in\N$ and $\lambda\geq 0$. Moreover, 
for $i\in I_0$ we have 
$$
a_i^T(x^k-\lambda u)+b_i=a_i^Tx^k+b_i\leq 0.
$$
If $i\notin I_0$, it follows from \eqref{bd_away} that 
$$
a_i^T(x^k-\lambda u)+b_i=a_i^Tx^k+b_i-\lambda a_i^Tu\leq-\varepsilon-\lambda a_i^Tu. 
$$
Thus, there exists $\bar\lambda>0$ such that $x^k-\lambda u\in\Omega$ for all $k\in\N$ 
and $\lambda\leq\bar\lambda$. Furthermore, since 
$$
\|x^k-\bar\lambda u\|^2=\|x^k\|^2+\bar\lambda\big(\bar\lambda-2\rho_ku^Tu^k\big),
$$
we can choose $\bar{k}\in\mathbb{N}$ such that 
$\|x^{\bar{k}}-\bar\lambda u\|<\|x^{\bar{k}}\|=\rho_{\bar{k}}$, which 
means that $x^{\bar{k}}-\bar\lambda u\in\Omega_{\rho_{\bar{k}}}$. 

Now, noting that the function $\rho\mapsto f(x_\rho)$ is nondecreasing, we have 
$$
f(x^0)\leq f(x^k)\leq
\underbrace{\rho_k\big(\rho_k(u^k)^TH_ju^k+c_j^Tu^k\big)+d_j}_{f_j(x^k)}
\leq\sup_{x\in\Omega}f_j(x)
$$
for all $k\in\N$ and $j\in[r]$, which implies 
\begin{equation}
\label{uHu0}
u^TH_ju=0.
\end{equation}
Therefore, 
$$
f_j(x^k+\lambda u)=f_j(x^k)+\lambda\big(2u^TH_jx^k+c_j^Tu\big),
$$
yielding $2u^TH_jx^k+c_j^Tu\leq 0$, because $f_j$ is bounded above on $\Omega$. 
Thus, 
$$
f_j(x^k-\lambda u)=f_j(x^k)-\lambda\big(2u^TH_jx^k+c_j^Tu\big)\geq f_j(x^k)
$$
for all $j\in[r]$, $k\in\N$ and $\lambda\geq 0$. This in turn, implies that 
$$
f(x^{\bar{k}}-\bar\lambda u)=\min_{j\in[r]}f_j(x^{\bar{k}}-\bar\lambda u)
=f_{\bar{j}}(x^{\bar{k}}-\bar\lambda u)\geq f_{\bar{j}}(x^{\bar{k}})\geq f(x^{\bar{k}}),
$$
which contradicts the property of $x^{\bar{k}}$, namely, a maximizer of $f$ on 
$\Omega_{\rho_{\bar{k}}}$ with minimum norm. This contradiction proves our claim 
in \eqref{xrho_inactive}. 

To finish the proof of the theorem, let us prove that 
$f(x_{\tilde\rho})=\displaystyle\sup_{x\in\Omega}f(x):=f^*$, again by contradiction. 
So, assume that $f(x_{\tilde\rho})<f^*$. Take $x'\in\Omega$ such that 
$f(x') > f(x_{\tilde\rho})$ and consider $\rho'>\max\{\|x'\|,\tilde\rho\}$. Thus, 
$$
f(x_{\rho'})\geq f(x') > f(x_{\tilde\rho}),
$$ 
implying that $x_{\rho'}\notin\Omega_{\tilde\rho}$ and hence 
$\tilde\rho<\|x_{\rho'}\|<\rho'$, where the second inequality holds in view 
of \eqref{xrho_inactive}. Defining $\hat\rho=\|x_{\rho'}\|$, we have 
$f(x_{\rho'})\geq f(x_{\hat\rho})\geq f(x_{\rho'})$, where the first inequality follows 
from the monotonicity of the function $\rho\mapsto f(x_\rho)$ and the second one holds 
because $x_{\rho'}\in\Omega_{\hat\rho}$. Moreover, as $\hat\rho>\tilde\rho$, 
using \eqref{xrho_inactive} again, we have $\|x_{\hat\rho}\|<\hat\rho=\|x_{\rho'}\|<\rho'$, 
contradicting the fact that $x_{\rho'}$ is a maximizer with minimum norm in 
$\Omega_{\rho'}$.
\qed\end{proof}

\begin{corollary}
\label{c:exist_quad}
Consider $H\in\R^{n\times n}$ symmetric, $c\in\R^n$, $d\in\R$ and the quadratic function 
$f:\R^n\to\R$ given by $f(x)=x^THx+c^Tx+d$. Suppose that $f$ is bounded below on the set 
$\Omega=\{x\in\R^n\mid Ax+b\leq 0\}$, defined in Theorem \ref{th:exist_piecewise_quad}. 
Then the problem 
$$
\begin{array}{cl}
\displaystyle{\rm minimize } & f(x) \\
{\rm subject\ to} & x\in\Omega
\end{array}
$$
has a global solution.
\end{corollary}


\subsection{Lagrangian relaxation for nonlinear programming}
\label{sec:dual}
Consider the nonlinear programming problem 
\begin{equation}
\label{primal_problem}
\begin{array}{cl}
\displaystyle\mathop{\rm minimize }  & f(x) \\
{\rm subject\ to } & g(x)\leq 0, \\
& h(x)=0, 
\end{array}
\end{equation}
where $f:\R^n\to\R$, $g:\R^n\to\R^m$ and $h:\R^n\to\R^p$ are continuously differentiable 
functions. The feasible set of this problem is denoted by 
$$
\Omega=\{x\in\R^n\mid g(x)\leq 0, h(x)=0\}
$$
and the Lagrangian function associated with \eqref{primal_problem} is 
$\ell:\R^n\times\R^m\times\R^p\to\R$ given by
$$
\ell(x,\mu,\lambda)=f(x)+\mu^Tg(x)+\lambda^Th(x).
$$

Defining the domain for the dual variables as 
$$
\mathcal{D}=\left\{(\mu,\lambda)\in\R_+^m\times\R^p
\mid\inf_{x\in\R^n}\ell(x,\mu,\lambda)>-\infty\right\}
$$
and the dual function $\theta:\mathcal{D}\to\R$ by 
$$
\theta(\mu,\lambda)=\displaystyle\inf_{x\in\R^n}\ell(x,\mu,\lambda),
$$ 
we have the dual problem for \eqref{primal_problem}, 
$$
\begin{array}{cl}
\displaystyle\mathop{\rm maximize }  & \theta(\mu,\lambda) \\
{\rm subject\ to } & (\mu,\lambda)\in\mathcal{D}.
\end{array}
$$
It is well known that 
\begin{equation}
\label{weak_duality}
\theta^*:=\sup_{(\mu,\lambda)\in\mathcal{D}}\theta(\mu,\lambda)
\leq\inf_{x\in\Omega}f(x):=f^*,
\end{equation}
the so-called {\em weak duality}.

Now we shall prove that if we dualize less constraints, the duality gap does not increase. 
For this purpose, consider natural numbers $\tilde{m}, \tilde{p}$ such that 
$0\leq\tilde{m}\leq m$ and $0\leq\tilde{p}\leq p$ and define 
$I=[\tilde{m}]$\footnote{Following the convention that if $\tilde{m}=0$, then 
$I=\emptyset$ and hence, there is no inequality to dualize. Analogously to $\tilde{p}=0$.}, 
$J=[\tilde{p}]$, 
$I^c=[m]\setminus{I}$, $J^c=[p]\setminus{J}$, the {\em relaxed} Lagrangian 
$\tilde\ell:\R^n\times\R^{\tilde{m}}\times\R^{\tilde{p}}\to\R$ given by
$$
\tilde\ell(x,\mu_I,\lambda_J)=f(x)+\mu_I^Tg_I(x)+\lambda_J^Th_J(x),
$$
the sets $\widetilde\Omega = \{x\in\R^n\mid g_{I^c}(x)\leq 0, h_{J^c}(x)=0\}$ and 
$$
\mathcal{\widetilde D}=
\left\{(\mu_I,\lambda_J)\in\R^{\tilde{m}}_+\times\R^{\tilde{p}}\mid\inf_{x\in\widetilde\Omega}
\tilde\ell(x,\mu_I,\lambda_J)>-\infty\right\},
$$
the function $\tilde\theta:\mathcal{\widetilde D}\to\R$ by 
$$
\tilde\theta(\mu_I,\lambda_J)=\inf_{x\in\widetilde\Omega}\tilde\ell(x,\mu_I,\lambda_J)
$$ 
and the {\em relaxed} dual problem 
\begin{equation}
\label{relaxed_dual_problem}
\begin{array}{cl}
\displaystyle\mathop{\rm maximize }  & \tilde\theta(\mu_I,\lambda_J) \\
{\rm subject\ to } & (\mu_I,\lambda_J)\in\mathcal{\widetilde D}.
\end{array}
\end{equation}

\begin{theorem}
\label{th:dual1}
The following statements are true. 
\begin{itemize}
\item[$(i)$] If $x\in\Omega$ and $(\mu,\lambda)\in\mathcal{D}$, then 
$(\mu_I,\lambda_J)\in\mathcal{\widetilde D}$ and 
$\theta(\mu,\lambda)\leq\tilde\theta(\mu_I,\lambda_J)\leq f(x)$; 
\item[$(ii)$] Defining 
$\tilde\theta^*=
\sup_{(\mu_I,\lambda_J)\in\mathcal{\widetilde D}}\tilde\theta(\mu_I,\lambda_J)$, 
we have $\theta^*\leq\tilde\theta^*\leq f^*$.
\end{itemize}
\end{theorem}
\begin{proof}
If $(\mu_I,\lambda_J)\notin\mathcal{\widetilde D}$, there would be a sequence 
$(x^k)_{k\in\mathbb{N}}\subset\widetilde\Omega$ such that 
$\tilde\ell(x^k,\mu_I,\lambda_J))\to-\infty$. Therefore, 
$$
\ell(x^k,\mu,\lambda)\leq f(x^k)+\mu_I^Tg_I(x^k)+\lambda_J^Th_J(x^k)=
\tilde\ell(x^k,\mu_I,\lambda_J)\to-\infty,
$$
a contradiction with the fact that $(\mu,\lambda)\in\mathcal{D}$. This proves that 
$(\mu_I,\lambda_J)\in\mathcal{\widetilde D}$ and hence there exists a sequence 
$(x^k)_{k\in\mathbb{N}}\subset\widetilde\Omega$ such that 
$$
\tilde\ell(x^k,\mu_I,\lambda_J)\to\tilde\theta(\mu_I,\lambda_J).
$$
Taking the limit, as $k\to\infty$, in the relation 
$$
\theta(\mu,\lambda)\leq\ell(x^k,\mu,\lambda)\leq\tilde\ell(x^k,\mu_I,\lambda_J),
$$
we obtain  $\theta(\mu,\lambda)\leq\tilde\theta(\mu_I,\lambda_J)$. Moreover, 
$$
\tilde\theta(\mu_I,\lambda_J)\leq\tilde\ell(x,\mu_I,\lambda_J)=
f(x)+\mu_I^Tg_I(x)+\lambda_J^Th_J(x)\leq f(x),
$$
proving $(i)$. Now, given $(\mu,\lambda)\in\mathcal{D}$, we have  
$\theta(\mu,\lambda)\leq\tilde\theta(\mu_I,\lambda_J)\leq\tilde\theta^*$, implying that 
$$
\theta^*=\sup_{(\mu,\lambda)\in\mathcal{D}}\theta(\mu,\lambda)\leq\tilde\theta^*.
$$
Finally, note that the inequality $\tilde\theta^*\leq f^*$ follows from 
$\tilde\theta(\mu_I,\lambda_J)\leq f(x)$.
\qed\end{proof}

\begin{remark}
\label{rm:strong}
In view of Theorem \ref{th:dual1}, if we have {\em strong duality}, in the sense that 
\eqref{weak_duality} holds as an equality, then the optimal value of 
problem \eqref{primal_problem} coincides with the optimal value of the relaxed dual 
problem, no matter how we dualize the problem, that is, which constraints we consider 
in the Lagrangian and which ones we leave apart. As a matter of fact, even in the 
absence of strong duality, if a feasible point of some relaxed dual problem has the 
same objective value as the primal objective function, then the corresponding points 
are global solutions of the two problems and KKT conditions will hold. This is 
established in the following result.
\end{remark}

\begin{theorem}
\label{th:str_dual}
Consider the primal and (relaxed) dual problems, \eqref{primal_problem} and 
\eqref{relaxed_dual_problem}, respectively. Suppose that $x^*\in\Omega$ and 
$(\mu_I^*,\lambda_J^*)\in\mathcal{\widetilde D}$ satisfy 
$\tilde\theta(\mu_I^*,\lambda_J^*)=f(x^*)$. Then $x^*$ is a global 
minimizer of \eqref{primal_problem} and $(\mu_I^*,\lambda_J^*)$ is a global maximizer of 
\eqref{relaxed_dual_problem}. Moreover, if the constraints $g_{I^c}$ and $h_{J^c}$ 
fulfill some constraint qualification, then there exists 
$(\mu_{I^c}^*,\lambda_{J^c}^*)\in\R^{m-\tilde{m}}\times\R^{p-\tilde{p}}$ such that 
$(x^*,\mu^*,\lambda^*)$ satisfies the KKT conditions for problem \eqref{primal_problem}.
\end{theorem}
\begin{proof}
Note first that, given $x\in\Omega$ and $(\mu_I,\lambda_J)\in\mathcal{\widetilde D}$, we have 
$$
\tilde\theta(\mu_I,\lambda_J)\leq\tilde\ell(x^*,\mu_I,\lambda_J)
\leq f(x^*)=\tilde\theta(\mu_I^*,\lambda_J^*)\leq f(x).
$$ 
This gives the two optimality statements. Now, from 
$$
f(x^*)=\tilde\theta(\mu_I^*,\lambda_J^*)\leq\tilde\ell(x^*,\mu_I^*,\lambda_J^*)
=f(x^*)+(\mu_I^*)^Tg_I(x^*)\leq f(x^*),
$$
we obtain $(\mu_I^*)^Tg_I(x^*)=0$ and 
$$
\inf_{x\in\widetilde\Omega}\tilde\ell(x,\mu_I^*,\lambda_J^*)=
\tilde\theta(\mu_I^*,\lambda_J^*)=\tilde\ell(x^*,\mu_I^*,\lambda_J^*).
$$
Therefore, by the constraint qualification condition we conclude that there exists 
$(\mu_{I^c}^*,\lambda_{J^c}^*)\in\R_+^{m-\tilde{m}}\times\R^{p-\tilde{p}}$ such that 
$$
\nabla_x\ell(x^*,\mu^*,\lambda^*)=
\nabla_x\tilde\ell(x^*,\mu_I^*,\lambda_J^*)+\sum_{i\in I^c}\mu_i^*\nabla g_i(x^*)+
\sum_{j\in J^c}\lambda_j^*\nabla h_j(x^*)=0
$$
and $(\mu_{I^c}^*)^\top g_{I^c}(x^*)=0$, proving the stationarity of 
$(x^*,\mu^*,\lambda^*)$ and completing the proof.
\qed\end{proof}


\subsection{Particular case of duality}
\label{sec:dual_empty}
Another situation where there is no duality gap occurs when the primal problem is 
infeasible and the constraints are linear. Consider the linear constrained optimization 
problem 
\begin{equation}
\label{linear_constr_problem}
\begin{array}{cl}
\displaystyle\mathop{\rm minimize }  & f(x) \\
{\rm subject\ to } & Ax\leq b , \\
& Cx=d, 
\end{array}
\end{equation}
where $f:\R^n\to\R$ is continuously differentiable, $A\in\R^{m\times n}$, 
$C\in\R^{p\times n}$, $b\in\R^m$ and $d\in\R^p$.

\begin{lemma}
\label{lm:primal_infeasible}
Suppose that the problem \eqref{linear_constr_problem} is infeasible and that $f$ is 
bounded below. Then the corresponding dual problem is unbounded above.
\end{lemma}
\begin{proof}
The Lagrangian function associated with \eqref{linear_constr_problem} is 
$\ell:\R^n\times\R^m\times\R^p\to\R$ given by
$$
\ell(x,\mu,\lambda)=f(x)+\mu^T(Ax-b)+\lambda^T(Cx-d).
$$
Let $f^*=\inf\{f(x)\mid{x}\in\R^n\}$. Using one of the variants of Farkas Lemma, either 
there exists $y\in\R_+^m$ such that 
$$
\left\{\begin{array}{c} A^Ty=0 \\ b^Ty<0, \end{array}\right.
$$
or there exists $z\in\R^p$ such that 
$$
\left\{\begin{array}{c} C^Tz=0 \\ d^Tz\neq 0. \end{array}\right.
$$
In the first case, defining the sequence $(\mu^k,\lambda^k)_{k\in\N}$ by $\mu^k=ky$ and 
$\lambda^k=0$, we have 
$$
\ell(x,\mu^k,\lambda^k)=f(x)-ky^Tb\geq{f^*}-ky^Tb.
$$
In the second case, defining $\mu^k=0$ and $\lambda^k=-k(d^Tz)z$, we have 
$$
\ell(x,\mu^k,\lambda^k)=f(x)+k(d^Tz)^2\geq{f^*}+k(d^Tz)^2.
$$
In any case, we can define a sequence $(\mu^k,\lambda^k)$ such that 
$$
\displaystyle\inf_{x\in\R^n}\ell(x,\mu^k,\lambda^k)\to\infty,
$$ 
completing the proof.
\qed\end{proof}


\section{The sparse quadratic problem}
\label{sec:sparse_problem}
Now we present an approach to deal with the original problem \eqref{problem_x}, which 
is stated again for convenience. 
$$
\begin{array}{cl}
\mathop{\rm minimize } & c^Tx+x^TQx+\frac{1}{\eta}\|x\|^2 \\ 
{\rm subject\ to } & Ax\leq b, \\ 
& \|x\|_0\leq s.
\end{array}
$$
Since $Q\in\R^{n\times n}$ is a symmetric positive semidefinite matrix, we can consider 
a spectral decomposition with its $n$ real eigenvalues 
$\lambda_1\geq\lambda_2\geq\cdots\geq\lambda_n\geq 0$ and their associated (orthonormal) 
eigenvectors $v_1, v_2, \ldots, v_n$. Thus, defining $V=(v_1\;\cdots\;v_n)$, 
$\Lambda={\rm diag}(\lambda_1,\ldots,\lambda_n)$ and $y=V^Tx$, we have $Q=V\Lambda V^T$ 
and hence, $x^TQx=y^T\Lambda y$. Therefore our problem can be reformulated as 
\begin{equation}
\label{problem_xy}
\begin{array}{cl}
\displaystyle\mathop{\rm minimize}_{(x,y)\in\,\R^n\times\R^n} & 
c^Tx+y^T\Lambda y+\textstyle\frac{1}{\eta}\|x\|^2 \\ 
{\rm subject\ to} & Ax \leq b, \\ 
& V^Tx-y = 0, \\ 
& \|x\|_0\leq s.
\end{array}
\end{equation}
We stress that although the inclusion of the auxiliary variable $y$ and 
corresponding constraint seems to be unnecessary, the usefulness of this 
strategy will become clear in the following discussion, where we shall be able to 
obtain a closed formula for the dual function of a certain quadratic problem (see 
Lemma \ref{lm:dual_fc}).

Now, in order to deal with the difficult constraint $\|x\|_0\leq s$ (the cardinality 
constraint) we make use of a variable $z\in\{0,1\}^n$ that counts how many (at least) 
and which components of $x$ must vanish. To be precise, given $z\in\{0,1\}^n$, define 
$I_z=\{j\mid{z_j}=0\}$. Thus, the condition $\|x\|_0\leq s$ is equivalent 
to $x_{I_z}=0$ for some $z$ satisfying $\sum\limits_{j=1}^{n}z_j=s$. So, we can 
decouple 
\begin{equation}
\label{decomposition}
\{x\in\R^n\mid\|x\|_0\leq s\}=
\bigcup_{\sum\limits_{j=1}^{n}z_j=s}\{x\in\R^n\mid{x_{I_z}}=0\}
\end{equation}
and rewrite problem \eqref{problem_xy} as the minimin problem 
\begin{equation}
\label{problem_minimin}
\begin{array}{ccl}
\displaystyle\mathop{\rm minimize}_{z\in\{0,1\}^n,\;\sum\limits_{j=1}^{n}z_j=s} & 
\displaystyle\mathop{\rm minimize}_{(x,y)\in\,\R^n\times\R^n} & 
c^Tx+y^T\Lambda y+\textstyle\frac{1}{\eta}\|x\|^2 \\ 
& {\rm subject\ to} & Ax\leq b, \\ 
& & V^Tx-y = 0, \\ 
& & x_{I_z}=0.
\end{array}
\end{equation}

In the next result we prove that the problem above has in fact a solution. 

\begin{lemma}
\label{lm:exist_inner_primal}
Given $z\in\{0,1\}^n$ such that $\sum\limits_{j=1}^{n}z_j=s$, consider 
the inner problem in \eqref{problem_minimin}. If the feasible set of this quadratic 
problem is nonempty, then it has a global minimizer $(x_z^*,y_z^*)$.
\end{lemma}
\begin{proof}
The proof follows directly from Corollary \ref{c:exist_quad}, since we have 
that the objective function is bounded below.
\qed\end{proof}

\begin{remark}
\label{rm:savings}
It should be noted that in \cite{VreugdenhilNguyenEftekhariEsfahani21} the authors use 
$\sum\limits_{j=1}^{n}z_j\leq s$ instead of $\sum\limits_{j=1}^{n}z_j=s$. However, by 
understanding that the decomposition \eqref{decomposition} holds with equality, our 
approach reduces the number of inner problems to be considered, namely, 
$\left(\begin{array}{c} n \\ s \end{array}\right)=\dfrac{n!}{(n-s)!s!}$. 
Figure \ref{fig1} illustrates the quantity of inner problems and the savings 
when $n=12$. For example, if $s=5$, we have $792$ problems satisfying the constraint 
$\sum\limits_{j=1}^{n}z_j=s$, while the constraint $\sum\limits_{j=1}^{n}z_j\leq s$ 
requires $794$ more problems to be solved. If $s=8$, the savings is 
dramatic, because the number is substantially reduced from $3797$ to $495$.
% \rmv{Boyd, 675} 
\begin{figure}[htbp]
\centering
\includegraphics[scale=.42]{fig/qt_outer1}
\caption{Graph of the function 
$\{0, 1, \ldots, 12\}\ni i\mapsto\left(\begin{array}{c} 12 \\ i \end{array}\right)$. 
Denoting $\Omega_1=\Big\{z\in\{0,1\}^n\mid\sum\limits_{j=1}^{n}z_j=s\Big\}$ and 
$\widetilde\Omega_1=\Big\{z\in\{0,1\}^n\mid\sum\limits_{j=1}^{n}z_j\leq s\Big\}$, we have 
$|\Omega_1|=\left(\begin{array}{c} n \\ s \end{array}\right)$, while 
$|\widetilde\Omega_1|= 
\sum\limits_{i=0}^{s}\left(\begin{array}{c} n \\ i \end{array}\right)$.}
\label{fig1}
\end{figure}
\end{remark}

Despite the savings pointing out in the remark above, we note that the price to 
pay for replacing the (challenging) cardinality constraint by the very simple ones, 
$x_{I_z}=0$, is still to solve a combinatorial number of quadratic convex 
problems. Nevertheless, as we shall see later, we will be able to find the optimal 
value of problem of \eqref{problem_minimin} without having to go through all the 
combinations. In order to do that, we first change each minimization problem to an 
equivalent maximization one. Thanks to the convexity of such inner problems, strong 
duality holds. Therefore, in view of Theorem \ref{th:dual1}, we 
conclude that the optimal value of each inner problem coincides with the one of the 
relaxed dual, no matter which constraints we dualize and which ones we keep out of 
the Lagrangian. 

For the sake of convenience, we shall scale the equality constraint involving 
the auxiliary variable $y$ by the square root of the eigenvalues. More precisely, in the 
inner problem of \eqref{problem_minimin}, we replace $V^Tx-y = 0$ by 
$\sqrt{\Lambda}(V^Tx-y) = 0$. Note that this change does not affect neither the optimal 
value of the problem nor the strong duality (and hence the dual value will be the same 
as well). However, it allows us to obtain a suitable expression for the dual function 
associated with this problem. Moreover, it will provide improvements on the numerical 
results to be shown in Sect. \ref{sec:numerical_experiments} (the gap between two problems 
minimax and maxmin).

We then consider the relaxed Lagrangian (without the constraint $x_{I_z}=0$ 
that comes from the cardinality) $\ell:\R^n\times\R^n\times\R^n\times\R^m\to\R$ given by 
$$
\begin{array}{ccl}
\ell(x,y,\alpha,\beta) & = & 
c^Tx+y^T\Lambda y+\textstyle\frac{1}{\eta}\|x\|^2+\alpha^T\sqrt{\Lambda}(V^Tx-y)+
\beta^T(Ax-b) \vspace{5pt} \\ 
& = & 
-b^T\beta+y^T\Lambda{y}-\alpha^T\sqrt{\Lambda}y+\gamma^Tx+\frac{1}{\eta}\|x\|^2,
\end{array}
$$
where $\gamma=c+V\sqrt{\Lambda}\alpha+A^T\beta$. 

\begin{lemma}
\label{lm:dual_fc}
Fixed $z\in\R^n$, consider the inner minimization problem in \eqref{problem_minimin} 
with the constraint $\sqrt{\Lambda}(V^Tx-y) = 0$ in place of $V^Tx-y = 0$. Then, the 
dual function associated with this problem is the function $\theta_z:\R^n\times\R^m\to\R$ 
given by 
\begin{equation}
\label{dual_function}
\theta_z(\alpha,\beta) = -b^T\beta - \dfrac{\|\alpha\|^2}{4} - 
\dfrac{\eta}{4}\gamma^T{\rm diag}(z)\gamma.
\end{equation}
\end{lemma}
\begin{proof}
Fixed $\alpha\in\R^n$ and $\beta\in\R^m$ we have 
$$
\begin{array}{ccl}
\theta_z(\alpha,\beta) & = & \displaystyle
\inf_{\substack{(x,y)\in\R^n\times\R^n \\ x_{I_z}=0}}\ell(x,y,\alpha,\beta) \vspace{5pt} \\ 
& = & \displaystyle
-b^T\beta+\inf_{y}\left(y^T\Lambda{y}-\alpha^T\sqrt{\Lambda}y\right)+
\inf_{x_{I_z}=\hspace{1pt}0}\left(\gamma^Tx+\frac{1}{\eta}\|x\|^2\right).
\end{array}
$$
We can easily see that $y^*$ defined by 
$y_i^*=\frac{1}{2}(\sqrt{\lambda_i})^{-1}\alpha_i$ if $\lambda_i>0$ and $y_i^*=0$, 
otherwise, is a stationary point for the convex function 
$y^T\Lambda{y}-\alpha^T\sqrt{\Lambda}y$. Thus, 
$$
\inf_{y}\left(y^T\Lambda{y}-\alpha^T\sqrt{\Lambda}y\right)=-\dfrac{1}{4}\|\alpha\|^2.
$$
Now, in order to compute the infimum in the variable $x$, we need to solve the optimality 
conditions 
$$
\frac{2}{\eta}x+\gamma+\sum\limits_{j\in I_z}\zeta_je_j = 0\quad\mbox{and}\quad x_{I_z}=0.
$$
If $i\notin I_z$, the $i$-th component of the vector $\sum\limits_{j\in I_z}\zeta_je_j$ 
is zero. Therefore, the solution is 
$x_i^*=-\frac{\eta}{2}\gamma_i$ for $i\notin I_z$ and $x_{I_z}^*=0$, giving 
$$
\inf_{x_{I_z}=\hspace{1pt}0}\left(\gamma^Tx+\frac{1}{\eta}\|x\|^2\right) = 
-\dfrac{\eta}{4}\gamma^T{\rm diag}(z)\gamma,
$$
completing the proof.
\qed\end{proof}

It is worth mentioning that thanks to the scaling we got the well conditioned term 
$\|\alpha\|^2$ in \eqref{dual_function}. If this scale were not used, this term would be 
$\alpha_\mathcal{J}^T\Lambda_\mathcal{J}^{-1}\alpha_\mathcal{J}$, where 
$\mathcal{J}=\{j\mid\lambda_j>0\}$. On the other hand, if we had scaled with $\Lambda$ 
instead of $\sqrt{\Lambda}$, it would be $\alpha^T\Lambda\alpha$.

In view of Lemma \ref{lm:dual_fc} and the comments before it, the 
problem \eqref{problem_minimin} can be reformulated as 
\begin{equation}
\label{problem_minimax}\begin{array}{ccl}
\displaystyle\mathop{\rm minimize}_{z\in\Omega_1} & 
\displaystyle\mathop{\rm maximize}_{(\alpha,\beta)\in\Omega_2} & 
\theta_z(\alpha,\beta),
\end{array}
\end{equation}
where 
$$
\Omega_1 = \bigg\{z\in\{0,1\}^n\mid\textstyle
\sum\limits_{j=1}^{n}z_j=s\bigg\}\quad\mbox{and}\quad
\Omega_2 = \R^n\times\R_+^m.
$$

Note that the equivalence between \eqref{problem_minimin} and \eqref{problem_minimax} 
holds even if for some $z$ the corresponding inner problem in \eqref{problem_minimin} 
is infeasible. Indeed, by Lemma \ref{lm:primal_infeasible}, in this case the maximization 
problem in \eqref{problem_minimax} is unbounded above and does not interfere with the 
outer minimization.

\begin{remark}
\label{rm:existence_minmin}
We also wish to stress that if the feasible set of problem \eqref{problem_x} is 
nonempty, then we can use Lemma \ref{lm:exist_inner_primal} to ensure that 
problem \eqref{problem_minimin} has an optimal solution, say, $(z^*,x^*,y^*)$. Thus, the 
constraint qualification (linearity of the constraints) ensures the existence of KKT 
multipliers $(\alpha^*,\beta^*)\in\,\R^n\times\R_+^m$. By the duality of convex problems, 
$(z^*,\alpha^*,\beta^*)$ is a solution of \eqref{problem_minimax}. Let us denote its 
optimal value $p^*$.
\end{remark}


\section{\Ademir{The maxmin problem}}
\label{sec:maxmin_problem}
\rmv{Tem muito ``now'' no paper.}
Now we follow a well known strategy used in the game theory, swapping the min and the 
max, obtaining the problem 
\begin{equation}
\label{problem_maxmin}
\begin{array}{ccl}
\displaystyle\mathop{\rm maximize}_{(\alpha,\beta)\in\Omega_2} & 
\displaystyle\mathop{\rm minimize}_{z\in\Omega_1} & 
\theta_z(\alpha,\beta).
\end{array}
\end{equation}
In general the problems \eqref{problem_minimax} and \eqref{problem_maxmin} are not 
equivalent, but we have the {\em minimax inequality}, namely, 
\begin{equation}
\label{minimax_inequality}
d^\star:=\sup_{(\alpha,\beta)\in\Omega_2}
\min_{z\in\Omega_1}
\theta_z(\alpha,\beta)\leq p^*.
\end{equation}
Indeed, given $z\in\Omega_1$ and $(\sigma,\rho)\in\Omega_2$, we have 
$$
\theta_z(\sigma,\rho)\leq 
\max_{(\alpha,\beta)\in\Omega_2}
\theta_z(\alpha,\beta).
$$
Thus, 
$$
\min_{z\in\Omega_1}
\theta_z(\sigma,\rho)\leq 
\min_{z\in\Omega_1}
\max_{(\alpha,\beta)\in\Omega_2}
\theta_z(\alpha,\beta) = p^*
$$
and hence $d^\star\leq{p^*}$. The minimax inequality \eqref{minimax_inequality} is 
sometimes referred to as ``weak duality'' since the problem \eqref{problem_maxmin} is 
considered the ``dual'' of \eqref{problem_minimax} in the game theory community. 

Another important fact here is that the ``$\sup$'' in \eqref{minimax_inequality} is in 
fact ``$\max$'', as established in the next result.

\begin{lemma}
\label{lm:exist_inner_dual}
The optimal value $d^\star$ of problem  \eqref{problem_maxmin} is attained at some 
point $(\alpha^\star,\beta^\star)\in\Omega_2$.
\end{lemma}
\begin{proof}
It follows directly from Theorem \ref{th:exist_piecewise_quad}, applied to the 
finite family of quadratic functions $\theta_z$, $z\in\Omega_1$.
\qed\end{proof}


\subsection{Solving the maxmin problem}
\label{sec:alg_maxmin}
In this section we state a subgradient 
algorithm \cite{BertsekasNedicOzdaglar03,Nesterov04} for solving the 
maxmin problem \eqref{problem_maxmin}. For this purpose, define 
$\xi:\R^n\times\R^m\to\R$ by 
\begin{equation}
\label{function:xi}
\xi(\alpha,\beta) = \min_{z\in\Omega_1}\theta_z(\alpha,\beta).
\end{equation}
We then aim to maximize $\xi$, which from Lemmas \ref{lm:gmzgm} and 
\ref{lm:max_gmzgm} is continuous, concave and piecewise quadratic.

Note that, given $(\bar\alpha,\bar\beta)\in\R^n\times\R^m$, it is easy to compute 
$\xi(\bar\alpha,\bar\beta)$. Indeed, consider 
$\bar\gamma=c+V\sqrt{\Lambda}\bar\alpha+A^T\bar\beta$ and the set of indices 
$\bar{J}=\{j_1, j_2,\ldots,j_s\}\subset[n]$ such that 
$|\bar\gamma_{j_1}|\geq|\bar\gamma_{j_2}|\geq\cdots
\geq|\bar\gamma_{j_s}|\geq|\bar\gamma_j|$ for $j\notin\bar{J}$. Defining $\bar{z}_j=1$ 
for $j\in\bar{J}$, $\bar{z}_j=0$ otherwise, we have 
\begin{equation}
\label{xi_value}
\xi(\bar\alpha,\bar\beta) = \theta_{\bar{z}}(\bar\alpha,\bar\beta)
\end{equation}
Moreover, in view of \cite[Lemma 3.1.10]{Nesterov04}, the gradient of the 
function\footnote{Note that $\theta_{\bar{z}}$ is one of the 
$\left(\begin{array}{c} n \\ s \end{array}\right)$ competing quadratic functions in the 
definition of $\xi$, and at $(\bar\alpha,\bar\beta)$ it wins.} 
$\theta_{\bar{z}}$ at $(\bar\alpha,\bar\beta)$ is a 
subgradient\footnote{Following the terminology used in \cite{Rockafellar}, 
even though terms like ``supergradients'' and ``superdifferential'' might be more 
appropriate for concave functions.} of $\xi$ at this point. So, defining 
\begin{equation}
\label{subgradient}
g_\alpha(\bar\alpha,\bar\beta)=
-\dfrac{\bar\alpha}{2}-\dfrac{\eta}{2}\sqrt{\Lambda}V^T{\rm diag}(\bar{z})\bar\gamma
\quad\mbox{and}\quad
g_\beta(\bar\alpha,\bar\beta)=
-b-\dfrac{\eta}{2}A{\rm diag}(\bar{z})\bar\gamma,
\end{equation}
we have $g(\bar\alpha,\bar\beta):=
\big(g_\alpha(\bar\alpha,\bar\beta),g_\beta(\bar\alpha,\bar\beta)\big)\in
\partial\xi(\bar\alpha,\bar\beta)$. 

Now we are ready to summarize the algorithm that will find the optimal value 
$d^\star$ of the problem \eqref{problem_maxmin}, which is written as 
\begin{equation}
\label{problem_max_xi}
\begin{array}{cl}
\displaystyle\mathop{\rm maximize}_{(\alpha,\beta)\in\Omega_2} & 
\xi(\alpha,\beta).
\end{array}
\end{equation}

\begin{algorithm}
\caption{Subgradient Max-Min (S2M)}
\label{alg:s2m}
\begin{tabbing}
\hspace*{7mm}\=\hspace*{7mm}\=\hspace*{7mm}\=\hspace*{7mm}\=
\hspace*{7mm}\=\hspace*{7mm}\=\kill
{\sc Initialization}: \+ \\ Choose an initial point 
$(\alpha_0,\beta_0)\in\Omega_2$ \\ 
Choose a stepsize sequence $(t_k)_{k\in\N}$ satisfying $t_k>0$, $t_k\to 0$ and 
$\sum\limits_{k=0}^{\infty}t_k=\infty$ \- \\ 
Set $k=0$ \\
{\sc Repeat}: \+ \\
1. Compute the objective value $\xi_k=\xi(\alpha_k,\beta_k)$ 
according to \eqref{xi_value} \\ 
2. Compute a subgradient $g_k=((g_\alpha)_k,(g_\beta)_k)\in\partial\xi(\alpha_k,\beta_k)$ 
according to \eqref{subgradient} \\ 
3. Project $(\alpha_k,\beta_k)+t_k\dfrac{g_k}{\|g_k\|}$ onto $\Omega_2$, 
obtaining \+ \\ 
3.1. $\alpha_{k+1}=\alpha_k+t_k\dfrac{(g_\alpha)_k}{\|g_k\|}$ \\ 
3.2. $\beta_{k+1}=\max\left\{0,\beta_k+t_k\dfrac{(g_\beta)_k}{\|g_k\|}\right\}$ \- \\ 
4. Set $k=k+1$
\end{tabbing} 
\end{algorithm}


\subsection{Convergence analysis of the S2M algorithm}
\label{sec:conv}
In this section we prove that, under mild conditions, 
Algorithm \ref{alg:s2m} produces a sequence that converges to the optimal 
value of problem \eqref{problem_max_xi}, namely, using the notation introduced in 
Lemma \ref{lm:exist_inner_dual}, 
$$
d^\star = \max_{(\alpha,\beta)\in\Omega_2}\xi(\alpha,\beta) = 
\xi(\alpha^\star,\beta^\star).
$$ 

\begin{theorem}
\label{th:conv}
Let $(\alpha_k,\beta_k)_{k\in\N}$ be a sequence generated by 
Algorithm \ref{alg:s2m}. Defining 
$\delta_k=\|(\alpha_k,\beta_k)-(\alpha^\star,\beta^\star)\|$, 
$\xi_k^\star=\max\{\xi_j\mid 0\leq j\leq k\}$, $M_k=\max\{\|g_j\|\mid 0\leq j\leq k\}$, 
$s_k=\sum\limits_{j=0}^{k}t_j$ and $\hat{s}_k=\sum\limits_{j=0}^{k}t_j^2$, we have 
\begin{equation}
\label{th:conv_eq1}
\delta_{k+1}^2\leq\delta_0^2+\hat{s}_k\quad\mbox{and}\quad
d^\star-\xi_k^\star\leq\dfrac{\delta_0^2+\hat{s}_k}{2s_k}M_k.
\end{equation}
Furthermore, if the sequence $(\hat{s}_k)_{k\in\N}$ is bounded, then so does 
$(\alpha_k,\beta_k)_{k\in\N}$ and hence, $\xi_k^\star\to d^\star$.
\end{theorem}
\begin{proof}
Denoting $\nu_j = (\alpha_j,\beta_j)+t_j\dfrac{g_j}{\|g_j\|}$, we have 
$(\alpha_{j+1},\beta_{j+1}) = {\rm proj}_{\Omega_2}(\nu_j)$. Thus, 
$$
\begin{array}{rcl}
\delta_{j+1}^2 & = & \|(\alpha_{j+1},\beta_{j+1})-(\alpha^\star,\beta^\star)\|^2 
\vspace{.1cm} \\ 
& = & \|{\rm proj}_{\Omega_2}(\nu_j)-(\alpha^\star,\beta^\star)\|^2
\vspace{.1cm} \\ 
& \leq & \|\nu_j-(\alpha^\star,\beta^\star)\|^2
\vspace{.1cm} \\ 
& = & \left\|(\alpha_j,\beta_j)-(\alpha^\star,\beta^\star)+t_j\dfrac{g_j}{\|g_j\|}\right\|^2
\vspace{.1cm} \\ 
& = & \delta_j^2 
+\dfrac{2t_j}{\|g_j\|}g_j^T(\alpha_j-\alpha^\star,\beta_j-\beta^\star)
+t_j^2,
\end{array}
$$
where the inequality follows from a basic property of the projection 
(see, for example, \cite[Lemma 3.1.5]{Nesterov04}). On the other hand, since 
$g_j\in\partial\xi(\alpha_j,\beta_j)$, there holds 
$$
\begin{array}{rcl}
d^\star & = & \xi(\alpha^\star,\beta^\star) 
\vspace{.1cm} \\ 
& \leq & \xi(\alpha_j,\beta_j) + g_j^T(\alpha^\star-\alpha_j,\beta^\star-\beta_j) 
\vspace{.1cm} \\ 
& = & \xi_j - g_j^T(\alpha_j-\alpha^\star,\beta_j-\beta^\star).
\end{array}
$$
Therefore, 
$$
\delta_{j+1}^2 \leq \delta_j^2 + \dfrac{2t_j}{\|g_j\|}(\xi_j-d^\star) + t_j^2
$$
Summing up the inequalities above for $j=0,\ldots,k$, we obtain 
$$
\delta_{k+1}^2\leq \delta_0^2+
2\sum\limits_{j=0}^k\dfrac{t_j}{\|g_j\|}(\xi_j-d^\star) + \hat{s}_k \leq 
\delta_0^2+\hat{s}_k
$$
proving the first inequality in \eqref{th:conv_eq1}. Furthermore, since 
$$
\dfrac{\xi_j-d^\star}{\|g_j\|} \leq \dfrac{\xi_k^\star-d^\star}{M_k},
$$
we have 
$$
0 \leq \delta_{k+1}^2 \leq 
\delta_0^2 + 2\dfrac{\xi_k^\star-d^\star}{M_k}\sum\limits_{j=0}^kt_j + \hat{s}_k = 
\delta_0^2 + 2\dfrac{\xi_k^\star-d^\star}{M_k}s_k + \hat{s}_k 
$$
giving the second part of \eqref{th:conv_eq1}. 

To finish the proof, note that the boundedness of $(\alpha_k,\beta_k)_{k\in\N}$ follows 
immediately from the first inequality in \eqref{th:conv_eq1} and the boundedness of 
$(\hat{s}_k)_{k\in\N}$. In this case, since $g_j$ is the gradient of a quadratic function 
(from a finite family of quadratic functions) at $(\alpha_j,\beta_j)$, we conclude that 
the sequence $(M_k)_{k\in\N}$ is bounded. This and the second inequality 
in \eqref{th:conv_eq1} imply that $\xi_k^\star\to d^\star$.
\qed\end{proof}

\begin{remark}
\label{rm:stepsize}
It can be shown that $\dfrac{\hat{s}_k}{s_k}\to 0$ even though the sequence 
$(\hat{s}_k)_{k\in\N}$ is not bounded. So, the convergence $\xi_k^\star\to d^\star$ 
actually depends on the boundedness of $(\|g_k\|)_{k\in\N}$, which is valid under a 
Lipschitz condition, for example. However, we did not used this classic assumption 
because the functions considered here are quadratic and hence not Lipschitz.
\end{remark}


\section{The proposed approach}
\label{sec:proposed_approach}
Now we have the ingredients to propose a strategy for solving the original 
cardinality-constrained problem \eqref{problem_x}. 

We first solve the maxmin problem \eqref{problem_maxmin}, obtaining its optimal 
value $d^\star$. Then, we start to solve the inner problems 
in \eqref{problem_minimax}, obtaining for each $z\in\Omega_1$ the optimal value $p_z$ 
and the corresponding maximizer pair $(\alpha_z,\beta_z)$. If for some $z\in\Omega_1$ 
we get $p_z=d^\star$, then $z=z^*$, $p_z=p^*$ and $(\alpha_z,\beta_z)=(\alpha^*,\beta^*)$, 
according to the notation of Remark \ref{rm:existence_minmin}. This means that we 
found the optimal value of problem of \eqref{problem_minimin} without the need to solve 
all the $N:=\left(\begin{array}{c} n \\ s \end{array}\right)$ problems. 

Finally, in order to obtain the primal solution $(x^*,y^*)$, recall the notation 
$\gamma=c+V\sqrt{\Lambda}\alpha+A^T\beta$ and consider the Lagrangian 
$$
\mathcal{L}(x,y,\alpha,\beta,\zeta) = 
-b^T\beta+y^T\Lambda{y}-\alpha^T\sqrt{\Lambda}y + 
\gamma^Tx+\frac{1}{\eta}\|x\|^2 + \zeta^Tx_{I_{z^*}}
$$
of the inner problem in \eqref{problem_minimin} with $z=z^*$. We then may invoke 
Theorem \ref{th:str_dual} to recover $(x^*,y^*)$ from $(\alpha^*,\beta^*)$. To be 
precise, this theorem ensures the existence of $\zeta^*\in\R^{|I_{z^*}|}$ such that 
$$
\nabla_{(x,y)}\mathcal{L}(x^*,y^*,\alpha^*,\beta^*,\zeta^*) = 0,
$$
which implies that 
$$
\frac{2}{\eta}x^*+\gamma^*+\sum\limits_{j\in I_{z^*}}\zeta_j^*e_j = 0.
$$
Therefore, recalling that $(x^*,y^*)$ meets the constraints 
in \eqref{problem_minimin}, we conclude that 
\begin{equation}
\label{primal_solution}
x_{I_{z^*}}^*=0\,,\;x_i^*=-\frac{\eta}{2}\gamma_i^*\mbox{ for }i\notin I_{z^*}
\mbox{ and } y^*=V^Tx^*.
\end{equation}

We summarize the discussion above in the following scheme, where the set $\Omega_1$ 
is represented by $\{z^1, z^2, \ldots, z^N\}$.

\begin{algorithm}
\caption{Sparse Quadratic Cardinality-Constrained (SQ2C)}
\label{alg:sq2c}
\begin{tabbing}
\hspace*{7mm}\=\hspace*{7mm}\=\hspace*{7mm}\=\hspace*{7mm}\=
\hspace*{7mm}\=\hspace*{7mm}\=\kill
{\sc Initialization}: \+ \\ Choose a tolerance $\tau>0$ \- \\
{\sc Phase I}: \+ \\ Solve the maxmin problem \eqref{problem_maxmin} using 
Algorithm \ref{alg:s2m} $\longrightarrow$ $d^\star$ \- \\ 
{\sc Phase II}: \+ \\ Set $k = 1$ \\
{\sc While $k \leq N$} \+ \\
Solve the inner problem in \eqref{problem_minimax} with $z = z^k$ $\longrightarrow$ 
$(p_k,\alpha_k,\beta_k)$  \\ 
{\sc If} ${(p_k-d^\star)}/{d^\star} < \tau$ \+ \\ 
$z^* = z^k$ \\ $p^* = p_k$ \\ $(\alpha^*,\beta^*) = (\alpha_k,\beta_k)$ \\ 
{\sc Break} \- \\ 
{\sc Else} \+ \\ 
$p_k^* = \min\{p_j\mid 1 \leq j \leq k\}$ \\
$j_* = {\rm argmin}\{p_j\mid 1 \leq j \leq k\}$ \\
$k=k+1$ \- \\ 
{\sc End} \- \\ 
{\sc End} \\ 
{\sc If} $k = N + 1$ \+ \\ 
$z^* = z^{j_*}$ \\ $p^* = p_N^*$ \\ $(\alpha^*,\beta^*) = (\alpha_{j_*},\beta_{j_*})$ \\ 
$\rho = 1$ \- \\ 
{\sc Else} \+ \\ 
$\rho = {k}/{N}$ \- \\ 
{\sc End} \\ 
Compute $(x^*,y^*)$ according to \eqref{primal_solution}
\end{tabbing}
\end{algorithm}

The percentage index $\rho$ measures the level of success of our strategy. The 
smaller $\rho$ is, the faster the convergence of the algorithm. In fact, as we shall 
see in the next section, in many instances, we can find the solution of our original 
problem \eqref{problem_x} by solving a small percentage of the inner (simple) 
quadratic problems in \eqref{problem_minimax}. 


\section{Numerical experiments}
\label{sec:numerical_experiments}
In this section we present numerical tests to demonstrate the effectiveness of the 
proposed approach, SQ2C algorithm. We generated several sets of problems, each set with 
$50$ random problems, according to the following patterns: 
\begin{itemize}
\item Dimension: $6 \leq n \leq 15$;
\item Sparsity level: 
$\left\lceil{n}/{4}\right\rceil \leq s \leq \left\lceil\dfrac{n}{2}\right\rceil$; 
\item Number of original linear constraints: $2 \leq m \leq 6$;
\item Regularization parameter: $\eta = \dfrac{\eta_1}{\eta_2}$, with $\eta_1\in[3]$ and 
$\eta_2\in[10]$;
\item Linear coefficient: $c\in\R^n$ such that $-2.5 \leq c_j \leq 2.5$, 
$j = 1, 2, \ldots, n$;
\item Orthogonal matrix: $V = {\rm qr}(B)\in\R^{n\times n}$, with 
$-2.5 \leq b_{ij} \leq 2.5$, $i,j = 1, 2, \ldots, n$;
\item Eigenvalues: $\lambda\in\R^n$ such that $0 \leq \lambda_j \leq 5$, 
$j = 1, 2, \ldots, n$, and, additionally, we forced some of them to be zero or a number 
very close to zero, like $10^{-10}$;
\item Matrix of the linear constraints: $A\in\R^{m\times n}$ such that 
$-2.5 \leq a_{ij} \leq 2.5$, $i = 1, 2, \ldots, m$, $j = 1, 2, \ldots, n$;
\item Vector of the linear constraints: $b\in\R^m$ such that 
$0 \leq b_i \leq 2.5$, $i = 1, 2, \ldots, m$.
\end{itemize}

The tests were conducted on a computer with \texttt{Intel Core i7} 3.00 GHz 
processor and 16.0 GB of RAM running \texttt{Linux Ubuntu 22.04.5 LTS}. The algorithms 
were implemented in \texttt{Julia 1.4.1}. For Algorithm \ref{alg:s2m}, we used the 
stepsize sequence given by $t_k = {1}/{\sqrt{k}}$ (as suggested in \cite{Nesterov04}) 
and the number of iterations bounded by $5000$. For solving the inner quadratic problem in 
Algorithm \ref{alg:sq2c}, we used the solver \texttt{Ipopt} \cite{WachterBiegler06} by 
means of the package \texttt{JuMP}, which is a domain-specific modeling language for 
mathematical optimization embedded in Julia. The tolerance was $\tau = 0.005$. 

Figures \ref{fig2} and \ref{fig3} illustrate a typical result obtained by running 
Algorithm~\ref{alg:sq2c} for all the sets of problems. Figure \ref{fig2} shows, for 
each instance of the $50$ problems, the optimal values of 
problems \eqref{problem_minimin} and \eqref{problem_maxmin}: $p^*$ represented 
by the red circles and $d^\star$ by the blue stars, respectively. As we can see, the 
gaps (dotted lines) are more evident in only a few ($\approx 10\%$) of the problems. 
For $\approx 16\%$ of them we see a small gap and in $\approx 32\%$ the gap is very small. 
Finally, we got gap ``zero'' (within a relative accuracy of 
${(p^*-d^\star)}/{d^\star} < \tau$) for $42\%$ of the problems, the green circles.

\begin{figure}[htbp]
\centering
\includegraphics[scale=.42]{fig/gap_values1}
\caption{Graphical illustration of the optimal values of 
problems \eqref{problem_minimin} and \eqref{problem_maxmin} for a set of $50$ randomly 
generated problems. The red circles represent $p^*$ and the blue stars represent 
$d^\star$. In only $10\%$ of the problems we see prominent gaps (dotted lines). We have 
a small gap in $\approx 16\%$ of the problems and in $\approx 32\%$ the gap is very small. 
The green circles represent $42\%$ of the problems, in which we got gap ``zero'' (within 
a relative accuracy of ${(p^*-d^\star)}/{d^\star} < \tau$).}
\label{fig2}
\end{figure}

In Figure \ref{fig3} we represent, for each instance of the $50$ problems, the 
percentage $\rho$ of the $N$ inner quadratic problems in \eqref{problem_minimax} needed 
to be solved in order to get the optimal primal value $p^*$. For some problems we see 
that this proportion is smaller than $10\%$. All the results depicted in these two 
figures are displayed in Table~\ref{tb:output}.

\begin{figure}[htbp]
\centering
\includegraphics[scale=.42]{fig/gap_zero_proportion1}
\caption{Graph of the function $\rho$, in terms of the $50$ problems, defined as the 
percentage of the inner quadratic problems in \eqref{problem_minimax} needed to be 
solved until we get the optimal primal value $p^*$. The green circles correspond to 
the problems for which we have identified $p^*$ without having to go through all the 
combinations. For example, for solving problem $50$, we needed to solve only $8.57\%$ 
of the $35$ inner problems, while for problem $49$, the demand was $37.7\%$ of $6435$ 
inner problems (see also Table~\ref{tb:output} for the complete and detailed 
informations). Note that for $24\%$ of the problems, the demand was less than $50\%$ 
of the total of combinations.}
\label{fig3}
\end{figure}

\begin{table}[htbp]
\centering
\begin{tabular}{ccccccccc}
\hline
Problem & $n$ & $s$ & $m$ & $\eta$ & $p^*$ & $d^\star$ & $N$ & $\rho$ \\ 
\hline
1 & 9 & 5 & 3 & 0.22 & -0.7778 & -0.7778 & 126 & 0.548\\2 & 6 & 3 & 5 & 0.29 & -0.5984 & -0.601 & 20 & 0.75\\3 & 15 & 5 & 2 & 0.11 & -0.6226 & -0.6261 & 3003 & 1.0\\4 & 12 & 3 & 4 & 0.12 & -0.3541 & -0.37 & 220 & 1.0\\5 & 7 & 2 & 6 & 0.38 & -0.7654 & -0.7659 & 21 & 0.0476\\6 & 8 & 3 & 6 & 0.6 & -1.031 & -1.059 & 56 & 1.0\\7 & 9 & 4 & 6 & 0.43 & -0.8615 & -0.8679 & 126 & 1.0\\8 & 8 & 2 & 5 & 2.0 & -3.482 & -4.208 & 28 & 1.0\\9 & 6 & 2 & 5 & 0.25 & -0.3216 & -0.3216 & 15 & 0.267\\10 & 15 & 5 & 5 & 0.33 & -1.35 & -1.376 & 3003 & 1.0\\11 & 7 & 3 & 4 & 0.25 & -0.53 & -0.5308 & 35 & 0.629\\12 & 7 & 3 & 2 & 0.2 & -0.5535 & -0.5535 & 35 & 0.714\\13 & 8 & 3 & 3 & 0.67 & -1.496 & -1.573 & 56 & 1.0\\14 & 8 & 3 & 4 & 0.33 & -0.6759 & -0.6773 & 56 & 0.0893\\15 & 12 & 5 & 5 & 1.0 & -3.01 & -3.39 & 792 & 1.0\\16 & 10 & 5 & 4 & 0.5 & -0.8783 & -0.8821 & 252 & 0.397\\17 & 11 & 3 & 4 & 0.14 & -0.5043 & -0.5152 & 165 & 1.0\\18 & 6 & 3 & 6 & 0.33 & -0.794 & -0.7952 & 20 & 1.0\\19 & 13 & 7 & 3 & 0.17 & -0.4063 & -0.4069 & 1716 & 0.307\\20 & 6 & 2 & 4 & 0.6 & -0.7423 & -0.7996 & 15 & 1.0\\21 & 7 & 4 & 6 & 0.29 & -0.4814 & -0.4815 & 35 & 0.286\\22 & 13 & 4 & 2 & 0.33 & -0.7267 & -0.7273 & 715 & 0.541\\23 & 8 & 4 & 6 & 0.6 & -1.1 & -1.1 & 70 & 0.357\\24 & 8 & 2 & 2 & 0.33 & -0.4688 & -0.4872 & 28 & 1.0\\25 & 15 & 8 & 4 & 0.5 & -2.258 & -2.262 & 6435 & 0.847\\26 & 10 & 5 & 6 & 0.14 & -0.4857 & -0.492 & 252 & 1.0\\27 & 7 & 2 & 2 & 1.0 & -0.9144 & -1.004 & 21 & 1.0\\28 & 6 & 2 & 3 & 0.67 & -0.7756 & -0.8673 & 15 & 1.0\\29 & 11 & 6 & 5 & 0.12 & -0.4273 & -0.4273 & 462 & 0.42\\30 & 15 & 4 & 3 & 2.0 & -2.655 & -3.054 & 1365 & 1.0\\31 & 7 & 3 & 4 & 0.25 & -0.6045 & -0.606 & 35 & 0.829\\32 & 9 & 3 & 6 & 0.5 & -1.055 & -1.055 & 84 & 0.536\\33 & 10 & 3 & 5 & 0.14 & -0.4324 & -0.4324 & 120 & 0.992\\34 & 11 & 6 & 5 & 0.75 & -2.983 & -3.044 & 462 & 1.0\\35 & 12 & 5 & 2 & 1.0 & -2.378 & -2.607 & 792 & 1.0\\36 & 7 & 4 & 5 & 2.0 & -2.715 & -2.736 & 35 & 1.0\\37 & 8 & 4 & 3 & 0.75 & -1.241 & -1.322 & 70 & 1.0\\38 & 9 & 5 & 3 & 0.3 & -0.6723 & -0.6723 & 126 & 0.175\\39 & 15 & 7 & 4 & 0.75 & -2.231 & -2.264 & 6435 & 1.0\\40 & 7 & 3 & 5 & 0.67 & -2.401 & -2.433 & 35 & 1.0\\41 & 9 & 5 & 2 & 0.2 & -0.3793 & -0.3793 & 126 & 0.492\\42 & 7 & 4 & 4 & 1.0 & -3.163 & -3.244 & 35 & 1.0\\43 & 7 & 2 & 6 & 0.14 & -0.2773 & -0.279 & 21 & 1.0\\44 & 12 & 5 & 5 & 0.1 & -0.3503 & -0.3639 & 792 & 1.0\\45 & 15 & 4 & 4 & 0.38 & -1.296 & -1.342 & 1365 & 1.0\\46 & 11 & 3 & 4 & 0.17 & -0.6117 & -0.6228 & 165 & 1.0\\47 & 13 & 5 & 6 & 0.14 & -0.6675 & -0.6721 & 1287 & 1.0\\48 & 7 & 2 & 5 & 2.0 & -1.743 & -2.589 & 21 & 1.0\\49 & 15 & 7 & 5 & 0.3 & -1.542 & -1.545 & 6435 & 0.377\\50 & 7 & 4 & 4 & 0.2 & -0.4378 & -0.4378 & 35 & 0.0857\\
\hline
\end{tabular}
\caption{Results obtained by running Algorithm~\ref{alg:sq2c} for a set of $50$ problems.}
\label{tb:output}
\end{table}

We can observe from Table \ref{tb:output} that the gap is more evident in only $5$ 
of the problems: $8$, $15$, $30$, $35$ and $48$. In contrast, for the remaining problems, 
even when the tolerance condition ${(p^*-d^\star)}/{d^\star} < \tau$ was not satisfied, 
that is, the ones for which $\rho = 1$ (the top black circles in Figure \ref{fig3}), 
we see that the value of $d^\star$ is close to $p^*$. This bolsters our confidence in the 
fact that solving the simple problem \eqref{problem_maxmin} may help us to find a 
solution of the original problem \eqref{problem_x}, overcoming the combinatorial 
difficulty.

As we have commented before Lemma \ref{lm:dual_fc}, the scaling of the constraint 
$V^Tx-y = 0$ in problem \eqref{problem_minimin} (using $\sqrt{\Lambda}(V^Tx-y) = 0$ 
instead) produces computational benefits. It is clear that it does not alter neither 
the optimal value nor the solution of the problem. However, it does change the function 
$\theta_z$ defined in \eqref{dual_function} and hence $\xi$, defined 
in \eqref{function:xi}. This in turn changes the problem~\eqref{problem_max_xi} and its 
solution $d^\star$. Despite we do not have a theoretical argument, it reduces the gap 
between $p^*$ and $d^\star$. Figure \ref{fig4} shows, for the same set of problems, 
these values and the corresponding gaps if we do not scale. In this case, denoting 
$\mathcal{J}=\{j\mid\lambda_j>0\}$, 
$D_\alpha=\{\alpha\in\R^n\mid\alpha_i=0, i\notin\mathcal{J}\}$, 
$\Lambda_\mathcal{J} = {\rm diag}(\lambda_\mathcal{J})$ and 
$\tilde\gamma=c+V\alpha+A^T\beta$, it can be proved that the dual function associated 
with the inner problem of \eqref{problem_minimin} is given by 
$$
-b^T\beta-\dfrac{1}{4}\alpha_\mathcal{J}^T\Lambda_\mathcal{J}^{-1}\alpha_\mathcal{J}-
\dfrac{\eta}{4}\tilde\gamma^T{\rm diag}(z)\tilde\gamma
$$
if $\alpha\in{D}_\alpha$, and $-\infty$ otherwise. As can be seen in the figure, the 
values of $d^\star$ are smaller and none of them provides a gap zero as in the tests 
shown before.

\begin{figure}[htbp]
\centering
\includegraphics[scale=.42]{fig/gap_values2}
\caption{Optimal values of problems \eqref{problem_minimin} and 
\eqref{problem_maxmin} for the same set of problems considered before, but without 
scaling. The red circles and blue stars represent $p^*$ and $d^\star$, respectively. 
Now, all the gaps fail to satisfy the tolerance condition 
${(p^*-d^\star)}/{d^\star} < \tau$.}
\label{fig4}
\end{figure}

Finally, it should be mentioned that if we had scaled with $\Lambda$ instead 
of $\sqrt{\Lambda}$, the dual function would be 
$$
-b^T\beta-\dfrac{1}{4}\alpha^T\Lambda\alpha-
\dfrac{\eta}{4}\hat\gamma^T{\rm diag}(z)\hat\gamma,
$$
where $\hat\gamma=c+V\Lambda\alpha+A^T\beta$. Although this approach yields results 
similar to the good ones obtained in the tests, we prefer the former because the term 
$\|\alpha\|^2$ is very well conditioned, while $\alpha^T\Lambda\alpha$ may be ill 
conditioned. 


\section{Conclusions}
\label{sec:conclusions} 
\Ademir{In this paper we have addressed the issue of dealing with the combinatorial 
nature induced by the cardinality constraint in optimization problems. Instead of using 
the continuous relaxation reformulation, we decomposed the feasible set of the problem 
into simpler sets, all of them satisfying the cardinality constraint. Then, y induced 
by this decomposition, we make use of an auxiliary and simple problem of maximizing a 
concave piecewise quadratic function. The solution of this problem, obtained by a 
subgradient method, is then used to find the solution of the original problem.

\rmv{
using a 
In this paper we propose a different approach in which the 
feasible set of the problem is decomposed into simpler sets, all of them meeting the 
cardinality constraint. Then, in order to overcome the combinatorial difficulty induced 
by this decomposition, we make use of an auxiliary and simple problem of maximizing a 
concave piecewise quadratic function. The solution of this problem, obtained by a 
subgradient method, is then used to find the solution of the original problem. 

In the case 
of regularized quadratic optimization, we are able to obtain closed forms for the 
duals, ... 

but this is subject of ongoing research.

We also establish some nonlinear optimization results in 
order to provide a rigorous mathematical analysis of the ideas involving the 
reformulation of problem, the proposed method and its convergence properties.

Não queremos vender uma estratégia para problemas grandes.}

The numerical experiments showed that our strategy may be successful for a reasonable 
number of problems, that is, solving the simple maxmin problem help us to find a solution 
of the original problem without the combinatorial difficulty.
}


\begin{acknowledgements}
The authors were partially supported by Brazilian agency Conselho Nacional de Desenvolvimento 
Científico e Tecnológico -- CNPq (Grant 403418/2023-2); \textbf{AAR} was also partially funded 
by CNPq (Grant 307987/2023-0). 
\end{acknowledgements}


\section*{Data Availability Statement}
The code (implemented in \texttt{Julia xxx.yyy.zzz}) and all the datasets generated 
and analyzed in this study are available from 
\url{https://github.com/RibeiroSachine/sparse_quadratic}.
\rmv{Temos que ver como criar um github (repositório).}


\section*{Conflict of interest}
All authors declare that they have no conflicts of interest.


\bibliographystyle{spmpsci}
\bibliography{/home/ademir/Ademir/bibfiles/references} 
% \bibliography{references} 



\end{document}


